{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8a4b519-7eb7-4126-9548-92be2c3c4e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aflah20082/anaconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.48k/9.48k [00:00<00:00, 7.46MB/s]\n",
      "Downloading readme: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.35k/2.35k [00:00<00:00, 2.64MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset toxigen-data/train to /home/aflah20082/.cache/huggingface/datasets/skg___toxigen-data/train/1.1.0/3dd39bc1508e10d3eebcca2f60948e1529149c78a24594fd929aaa1f1bda74d0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 165M/165M [00:09<00:00, 17.9MB/s]\n",
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset toxigen-data downloaded and prepared to /home/aflah20082/.cache/huggingface/datasets/skg___toxigen-data/train/1.1.0/3dd39bc1508e10d3eebcca2f60948e1529149c78a24594fd929aaa1f1bda74d0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 378.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset toxigen-data/annotated to /home/aflah20082/.cache/huggingface/datasets/skg___toxigen-data/annotated/1.1.0/3dd39bc1508e10d3eebcca2f60948e1529149c78a24594fd929aaa1f1bda74d0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|                                                                                                                                                              | 0/2 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                                                                                                                                                               | 0.00/349k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   5%|███████                                                                                                                                               | 16.4k/349k [00:00<00:03, 91.2kB/s]\u001b[A\n",
      "Downloading data:  15%|██████████████████████▏                                                                                                                                | 51.2k/349k [00:00<00:01, 150kB/s]\u001b[A\n",
      "Downloading data:  40%|████████████████████████████████████████████████████████████▎                                                                                           | 138k/349k [00:00<00:00, 297kB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 349k/349k [00:00<00:00, 475kB/s]\u001b[A\n",
      "Downloading data files:  50%|███████████████████████████████████████████████████████████████████████████                                                                           | 1/2 [00:04<00:04,  4.34s/it]\n",
      "Downloading data:   0%|                                                                                                                                                              | 0.00/3.05M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|▍                                                                                                                                                    | 9.22k/3.05M [00:00<00:59, 51.0kB/s]\u001b[A\n",
      "Downloading data:   1%|██                                                                                                                                                    | 43.0k/3.05M [00:00<00:23, 130kB/s]\u001b[A\n",
      "Downloading data:   4%|██████▍                                                                                                                                                | 130k/3.05M [00:00<00:10, 288kB/s]\u001b[A\n",
      "Downloading data:   9%|██████████████▏                                                                                                                                        | 287k/3.05M [00:00<00:05, 511kB/s]\u001b[A\n",
      "Downloading data:  20%|██████████████████████████████▌                                                                                                                        | 617k/3.05M [00:00<00:02, 977kB/s]\u001b[A\n",
      "Downloading data:  41%|████████████████████████████████████████████████████████████▋                                                                                        | 1.24M/3.05M [00:01<00:01, 1.80MB/s]\u001b[A\n",
      "Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3.05M/3.05M [00:01<00:00, 2.34MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.48s/it]\n",
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset toxigen-data downloaded and prepared to /home/aflah20082/.cache/huggingface/datasets/skg___toxigen-data/annotated/1.1.0/3dd39bc1508e10d3eebcca2f60948e1529149c78a24594fd929aaa1f1bda74d0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 956.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "TG_data = load_dataset(\"skg/toxigen-data\", name=\"train\", use_auth_token=\"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\") # 250k training examples\n",
    "TG_annotations = load_dataset(\"skg/toxigen-data\", name=\"annotated\", use_auth_token=\"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\") # Human study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59033afc-b810-457c-a3df-32931eeba1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'target_group', 'factual?', 'ingroup_effect', 'lewd', 'framing', 'predicted_group', 'stereotyping', 'intent', 'toxicity_ai', 'toxicity_human', 'predicted_author', 'actual_method'],\n",
       "    num_rows: 8960\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TG_annotations['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d8a5c4-9ee6-4ff8-a768-2f335214b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = TG_data['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6f7753-abe5-4b28-8a59-bc39ab7bb5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['generation', 'group', 'prompt_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3282c4c4-599d-4d3c-ae85-3f6914e5d3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generation</th>\n",
       "      <th>group</th>\n",
       "      <th>prompt_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if youre white, you should be proud to be whit...</td>\n",
       "      <td>black</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the best thing that could have happened to the...</td>\n",
       "      <td>black</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>theres no hope for the future because theres n...</td>\n",
       "      <td>black</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if youre white, you should be proud of it.</td>\n",
       "      <td>black</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no matter how much money or power you have, if...</td>\n",
       "      <td>black</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250946</th>\n",
       "      <td>My Chinese name is ai-lun (�媈礍) .\\n- My famil...</td>\n",
       "      <td>women</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250947</th>\n",
       "      <td>I can't believe they won</td>\n",
       "      <td>women</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250948</th>\n",
       "      <td>fín had a reputation as a musicians' city</td>\n",
       "      <td>women</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250949</th>\n",
       "      <td>I am thinking about getting a pet dog</td>\n",
       "      <td>women</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250950</th>\n",
       "      <td>it is dangerous to drive a car on windy roads</td>\n",
       "      <td>women</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250951 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               generation  group  prompt_label\n",
       "0       if youre white, you should be proud to be whit...  black             1\n",
       "1       the best thing that could have happened to the...  black             1\n",
       "2       theres no hope for the future because theres n...  black             1\n",
       "3              if youre white, you should be proud of it.  black             1\n",
       "4       no matter how much money or power you have, if...  black             1\n",
       "...                                                   ...    ...           ...\n",
       "250946   My Chinese name is ai-lun (�媈礍) .\\n- My famil...  women             0\n",
       "250947                           I can't believe they won  women             0\n",
       "250948          fín had a reputation as a musicians' city  women             0\n",
       "250949              I am thinking about getting a pet dog  women             0\n",
       "250950      it is dangerous to drive a car on windy roads  women             0\n",
       "\n",
       "[250951 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35059808-b363-48b9-8ad1-4462e5293cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleansetext\n",
    "from cleansetext.pipeline import Pipeline\n",
    "from cleansetext.steps import *\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tk = TweetTokenizer()\n",
    "\n",
    "# Create a pipeline with a list of preprocessing steps\n",
    "pipeline = Pipeline([\n",
    "    RemoveEmojis(),\n",
    "    RemoveAllPunctuations(),\n",
    "    RemoveTokensWithOnlyPunctuations(),\n",
    "    ReplaceURLsandHTMLTags(),\n",
    "    ReplaceUsernames(),\n",
    "    RemoveWhiteSpaceOrChunksOfWhiteSpace()\n",
    "], track_diffs=False)\n",
    "\n",
    "def apply_preprocessing(text):\n",
    "    text = tk.tokenize(text)\n",
    "    text = pipeline.process(text)\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd049e0-3369-48ea-be82-ec48746ec0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['generation'] = df_train['generation'].apply(lambda x: apply_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b970d655-bcfa-426d-8d31-9b5ff4d0d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_label = df_train[['generation', 'prompt_label']]\n",
    "df_train_group = df_train[['generation', 'group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8d3275a-748d-4e03-893d-535729529424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train_label_train, df_train_label_test = train_test_split(df_train_label, random_state=42, test_size=0.25)\n",
    "df_train_group_train, df_train_group_test = train_test_split(df_train_group, random_state=42, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91865fd5-54a0-4acc-9dbd-3e7bd86f413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = '/home/aflah20082/Probing_Experiments/Adding_Classification_Heads_In_Between/prepared_data/'\n",
    "\n",
    "df_train_label_train.to_csv(SAVE_PATH + 'toxigen_label_train.csv')\n",
    "df_train_label_test.to_csv(SAVE_PATH + 'toxigen_label_test.csv')\n",
    "\n",
    "df_train_group_train.to_csv(SAVE_PATH + 'toxigen_group_train.csv')\n",
    "df_train_group_test.to_csv(SAVE_PATH + 'toxigen_group_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc007b6d-9bb8-42a2-8419-3432f117bae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
